{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment 3 Naïve Bayes and Sentiment Classification and Logistic Regression\n",
        "Instructions\n",
        "* Read the following Chapter 4: Naive Bayes and Sentiment Classification. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "* Read the following Chapter 5: Logistic Regression. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "\n",
        "Summary\n",
        "Classification is one of the most important tasks of NLP and in machine learning. In NLP it often means the task of text categorization for both sentiment analysis, spam detection, and topic modeling. Naïve Bayes is often one of the first classification algorithms defined in NLP.  The intuition behind a classifier is lies at the underlying probability inferred by the Bayesian Inference, which uses Baye’s rule and conditional probabilities.\n",
        "\n",
        "Here’s a reminder on Baye’s Rule:\n",
        "P(y)=P(x)P(x)/(P(y))\n",
        "\n",
        "We are saying “what is the probability of x given y”. Naïve Bayes is a generative model because there is an input that helps the model determine what the output could be. Said differently, “to train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it.” [6]\n",
        "\n",
        "So in the case of Naïve Bayes, we say given some word, what should be the class of the current word we are assessing? Contrastingly, discriminative models such as logistic regression, learn from features provided to the algorithm and then determine or predict what the class is. [7]\n",
        "\n",
        "\n",
        "With Naïve Bayes, the assumption is that the probabilities are independent. We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n",
        "\n",
        "Back to bag of words. With bag of words, we assume that the position of the words are not relevant -- that dependency or context in the word phrase or sentence doesn’t matter. Relatedly, the naive Bayes assumption implies that the conditional probabilities are independent -- a rather strange assumption to make for words in a sentence! The equation for the naive Bayes classifier is outlined below:\n",
        "\n",
        "You can use Naive Bayes by creating an index of words and walking through every word position in a test or corpus.\n"
      ],
      "metadata": {
        "id": "liqKR9Vk9RSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.\n",
        "\n",
        "For this Assignment, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/\n",
        "\n",
        "You may work alone or in a group on this project.  You're welcome to use any tools or approach that you like.  Due before our next meetup. Starter code provided below.\n",
        "\n",
        "Test example is provided at the end."
      ],
      "metadata": {
        "id": "CIBB2IVT92Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries you may wish to use"
      ],
      "metadata": {
        "id": "c8sZQL-a-cHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import makedirs, path, remove, rename, rmdir\n",
        "from tarfile import open as open_tar\n",
        "from shutil import rmtree\n",
        "from urllib import request, parse\n",
        "from glob import glob\n",
        "from os import path\n",
        "from re import sub\n",
        "from email import message_from_file\n",
        "from glob import glob\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import gc"
      ],
      "metadata": {
        "id": "NHiCf9fi9103"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download corpus using the following functions\n",
        "\n",
        "Note: you may need to mount your drive on google then run this location. See previous exercises."
      ],
      "metadata": {
        "id": "uObO057u-Rne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C4fIGS9-8wce"
      },
      "outputs": [],
      "source": [
        "def download_corpus(dataset_dir: str = 'data'):\n",
        "    base_url = 'https://spamassassin.apache.org'\n",
        "    corpus_path = 'old/publiccorpus'\n",
        "    files = {\n",
        "        '20021010_easy_ham.tar.bz2': 'ham',\n",
        "        '20021010_hard_ham.tar.bz2': 'ham',\n",
        "        '20021010_spam.tar.bz2': 'spam',\n",
        "        '20030228_easy_ham.tar.bz2': 'ham',\n",
        "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
        "        '20030228_hard_ham.tar.bz2': 'ham',\n",
        "        '20030228_spam.tar.bz2': 'spam',\n",
        "        '20030228_spam_2.tar.bz2': 'spam',\n",
        "        '20050311_spam_2.tar.bz2': 'spam' }\n",
        "\n",
        "    #creates the folders: downloads, ham and spam\n",
        "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
        "    ham_dir = path.join(dataset_dir, 'ham')\n",
        "    spam_dir = path.join(dataset_dir, 'spam')\n",
        "\n",
        "    makedirs(downloads_dir, exist_ok=True)\n",
        "    makedirs(ham_dir, exist_ok=True)\n",
        "    makedirs(spam_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    for file, spam_or_ham in files.items():\n",
        "        # download files from URL of each specific .bz2 file\n",
        "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
        "        tar_filename = path.join(downloads_dir, file)\n",
        "        request.urlretrieve(url, tar_filename)\n",
        "\n",
        "        #list e-mails in the compressed .bz2 file\n",
        "        emails = []\n",
        "        with open_tar(tar_filename) as tar:\n",
        "            tar.extractall(path=downloads_dir)\n",
        "            for tarinfo in tar:\n",
        "                if len(tarinfo.name.split('/')) > 1:\n",
        "                    emails.append(tarinfo.name)\n",
        "\n",
        "        # move e-mails to ham or spam directory\n",
        "        for email in emails:\n",
        "            directory, filename = email.split('/')\n",
        "            directory = path.join(downloads_dir, directory)\n",
        "\n",
        "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
        "                rename(path.join(directory, filename),\n",
        "                   path.join(dataset_dir, spam_or_ham, filename))\n",
        "\n",
        "        rmtree(directory)\n",
        "\n",
        "download_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n"
      ],
      "metadata": {
        "id": "MUmHvbCn-o3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
        "ham_dir = path.join('data', 'ham')\n",
        "spam_dir = path.join('data', 'spam')\n",
        "\n",
        "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))\n",
        "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))"
      ],
      "metadata": {
        "id": "Cx-Blo33-oM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b6eca6-fcd2-44ec-f484-6685e6a4bbc0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Non-Spam E-mails: 6952\n",
            "\n",
            "Number of Spam E-mails: 2399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your classifier below"
      ],
      "metadata": {
        "id": "v3fSuJ0G_jNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### START CODE ####\n",
        "\"\"\" Enter code here\"\"\"\n",
        "# Import the required libraries\n",
        "import os\n",
        "import glob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "# Import the NLTK libraries\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "# Download the NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Read all the emails from the directory\n",
        "def read_emails_dir(directory):\n",
        "    emails = []\n",
        "    # iterate through the files\n",
        "    for file_path in glob.glob(os.path.join(directory, '*')):\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            content = file.read()\n",
        "            # append all the emails here\n",
        "            emails.append((content, directory.split('/')[-1]))\n",
        "    return emails\n",
        "\n",
        "# Read the ham and spam emails\n",
        "ham_emails = read_emails_dir('data/ham')\n",
        "spam_emails = read_emails_dir('data/spam')\n",
        "\n",
        "# Concatenate the list of ham and spam emails\n",
        "email_collection = ham_emails + spam_emails\n",
        "\n",
        "# The feature extractor function\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    return {word: True for word in words}\n",
        "\n",
        "# The feature sets are created\n",
        "feature_sets = [(document_features(word_tokenize(email)), category) for (email, category) in email_collection]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_set, test_set = train_test_split(feature_sets, test_size=0.2, random_state=42)\n",
        "\n",
        "# The Naive Bayes classifier is trained here\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# The classifier evaluation\n",
        "accuracy_score = accuracy(classifier, test_set)\n",
        "print(f'The Accuracy Score generated: {accuracy_score:.2%}')\n",
        "\n",
        "\n",
        "#### END CODE ####"
      ],
      "metadata": {
        "id": "MO9eKbq8_llJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be57ac6-beb8-4169-90c1-ed924f62f128"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy Score generated: 99.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following email is a test email. You can take this and test your classifier to see if it predicts spam or not.\n"
      ],
      "metadata": {
        "id": "J5WtArb0_IMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email = \"\"\"\n",
        "Subject: Get Rich Quick!\n",
        "\n",
        "Dear Friend,\n",
        "\n",
        "Congratulations! You've been selected to participate in an exclusive opportunity to make thousands of dollars from the comfort of your own home. Our revolutionary system guarantees quick and easy cash with minimal effort.\n",
        "\n",
        "No more struggling to pay bills or worrying about financial security. With our proven method, you can start earning massive amounts of money in no time.\n",
        "\n",
        "Here's what some of our satisfied customers have to say:\n",
        "- \"I was skeptical at first, but I'm now living my dream life thanks to this incredible system!\" - John S.\n",
        "- \"I never thought making money online could be this simple. It's changed my life!\" - Sarah L.\n",
        "\n",
        "Don't miss out on this limited-time offer. Act now to secure your spot and start enjoying a life of financial freedom.\n",
        "\n",
        "Click the link below to get started:\n",
        "www.getrichquick.com\n",
        "\n",
        "Remember, this opportunity is exclusive and won't last long. Take control of your financial future today!\n",
        "\n",
        "Best regards,\n",
        "The Get Rich Quick Team\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "_lvIjkRW_O8e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The feature Vectorization\n",
        "spam_email_features = document_features(word_tokenize(spam_email))\n",
        "\n",
        "# test the classifier to see the prediction\n",
        "# Predict the class of the new email\n",
        "predict_email = classifier.classify(spam_email_features)\n",
        "print('\\n The Predicted Class for the test Email:', predict_email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXoVamNFz4wS",
        "outputId": "55f872f1-398f-4fdb-e37c-f88728dd3e43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " The Predicted Class for the test Email: spam\n"
          ]
        }
      ]
    }
  ]
}